{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Math of Intelligence Step #1\n",
    "==\n",
    "***by Yoo Bin Kim. 2019-01-14.***\n",
    "\n",
    "- based on Siraj Raval's Youtube Channel. \n",
    "- 참고) https://www.youtube.com/watch?v=xRJCOz3AfYY&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 기본적인 머신러닝(기계학습)의 맛보기를 마쳤으니, \n",
    "- 사용했던 알고리즘들이 실제로 어떻게 수학적으로 이루어진 것인지 이해하여 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알고리즘의 기본 종류\n",
    "==\n",
    "    1. Regression(회귀 모델)\n",
    "        - Linear Regression (선형회귀)\n",
    "        - 등등\n",
    "\n",
    "    2. Classification (분류 모델)\n",
    "        - Logistic Regression (로지스틱회귀)\n",
    "        - Decisions Trees (결정 트리)\n",
    "        - Perceptron\n",
    "        - Naive Bayes\n",
    "        - 등등\n",
    "        \n",
    "    3. Clustering (클러스터 분석)\n",
    "        - K-Means\n",
    "        - KD Trees\n",
    "        - Agglomerative/Hierarchical\n",
    "        - 등등\n",
    "        \n",
    "- 아직 세부 각각은 모르더라도, 1,2,3 종류에 따른 특성을 알아야 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Gradient Descent (경사 하강법 / 선형 회귀)\n",
    "==\n",
    "영상) https://www.youtube.com/watch?v=xRJCOz3AfYY&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아무 셈플 데이터 (cycle_data.csv)를 가져옵니다. \n",
    "- 데이터 링크) https://drive.google.com/open?id=1fNeloEDXMRIobyKXmvlMxQwvVNCeYTnt\n",
    "- 이 경우에는 x 데이터는 자건거선수가 이동한 거리(Km), 그리고 y는 그만큼 소모한 칼로리(kcal)이라고 합니다. \n",
    "    \n",
    "- \"목표\"는 이 데이터를 Gradient Descent로 최적화하여 이동한 거리에 따른 칼로리의 상관관계를 알아내는 것 입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = mx + b\n",
    "# m 은 경사 , b 는 당연히 y-절편입니다.\n",
    "\n",
    "# 먼저 최적화된 y=mx+b 공식을 찾아내는 것이 목적입니다. \n",
    "# 그런 함수들을 만들어 보겠습니다. \n",
    "\n",
    "# 먼저 주어진 선에 대한 오류값을 계산하는 함수를 만들어보겠습니다. \n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    \n",
    "    # 위의 함수에서 랜덤한 m(slope)와 b(y-inter)를 받아왔습니다.\n",
    "    # 그럼 이 '선'이 얼마나 빗나가 있는지를 계산해봐야 하겠지요?\n",
    "    \n",
    "    # 때문에 모든 포인트에 대해 SSE (Sum of Squares Error)를 구합니다. \n",
    "    for i in range(0, len(points)): # 1. 모든 데이터 포인트에 대해\n",
    "        x = points[i, 0]\n",
    "        # 2. 데이터포인트의 y값에서 \n",
    "        y = points[i, 1] \n",
    "        # 3. 주어진 선의 y값을 빼고 제곱한 것을 다 더한다면 그것이 이 선의 '오류'가 됩니다. \n",
    "        totalError += (y - (m * x + b)) ** 2  \n",
    "    return totalError / float(len(points)) \n",
    "    # 이 부분 이해가 안갈 시 영상이나 SSE에 대해 더 조사하세요. (참고 http://igija.tistory.com/256)\n",
    "    # 결과적으로 회귀식 추정 y의 편차제곱의 합입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과적으로 경사 하강은 편미분 업데이팅의 반복입니다. \n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b # 시작 y절편\n",
    "    m = starting_m # 시작 경사 \n",
    "    for i in range(num_iterations): # 정해진 많은 횟수만큼 \n",
    "        b, m = step_gradient(b, m, array(points), learning_rate) # 경사 하강을 진행\n",
    "    return [b, m]\n",
    "\n",
    "# 경사 하강 함수, 즉 편미분을 진행합니다.\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # b에 대한 편미분 (Power Rule 이용)\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        # m에 대한 편미분 (Power Rule 이용)\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "        \n",
    "    # 그리고 그 값을 계속 Update하여, 경사 하강 (Gradient Descent)을 진행되는 것 입니다.     \n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경사 하강법을 at b = 0, m = 0, error = 5565.107834483211 부터 시작합니다.\n",
      "실행 중...\n",
      "10000 횟수를 통해 b = 0.6078985997054931, m = 1.4675440436333027, error = 112.31533427075733 만큼 최적화가 진행되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "# 상기 만들어진 함수들을 실행하여 경사 하강법으로 주어진 데이터에 대한 최적화를 진행합니다.\n",
    "def run():\n",
    "    \n",
    "    points = genfromtxt(\"cycle_data.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 10000\n",
    "    print (\"경사 하강법을 at b = {}, m = {}, error = {} 부터 시작합니다.\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print (\"실행 중...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print (\"{} 횟수를 통해 b = {}, m = {}, error = {} 만큼 최적화가 진행되었습니다.\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 횟수를 늘리더라도 이미 하강이 완료된 시점에서 당연히 더 이상 에러가 줄지는 않습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Support Vector Machines (SVM) Classification (분류 모델)\n",
    "==\n",
    "\n",
    "- 영상) https://www.youtube.com/watch?v=g8D5YL6cOSE&index=2&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D\n",
    "- 주로 데이터 두 가지 혹은 그 이상의 상태로 분류 하는 용도로 사용되나,\n",
    "- 시계열 자료를 회귀시켜 미래의 값을 예상거나, Outlier(특이사항)을 감지하는 용도로도 사용되기도 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 경우에는 Supervised Classification Learning, 즉 Label된 데이터를 Mapping하여 (함수로 만들어) \n",
    "- 새로운 데이터가 어디에 속하게 되는지 학습하게 하여 보겠습니다. (대표적인 예로 MNIST)\n",
    "- 일반적으로 1000개 이하의 데이터에 효과적이며, 그 이상은 다른 알고리즘 (Random Forests, Deep NN)등이 더 효과적입니다.\n",
    "- 다만 데이터에 따라 SVM이 더 분류에 효과적일 수 있기 때문에 가능하면 쉽고 편하게 SVM을 쓰는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 가지 이상의 레이블데이터 사이에 이를 분류하는 선(Hyperplane)의 거리를 Maximize하는 것이 목적입니다.\n",
    "- 선을 면(plane)으로 표현한 이유는, 3차원 이상에서는 선이 Hyperplane이 되기 때문입니다.\n",
    "- 참고) 굳이 2차원을 사용하지 않는 경우는 대부분 2차원에서는 선이 너무 복잡해지는데,\n",
    "- 이를 Feature Space, 즉 3차원 그 이상으로 만들면 평면인 plane으로 데이터를 쉽게 나눠버릴 수 있게 됩니다. \n",
    "- 참고) https://github.com/llSourcell/Classifying_Data_Using_a_Support_Vector_Machine/blob/master/support_vector_machine_lesson.ipynb 의 사진이 효과적으로 표현하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f14c4375cf8>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 일반적으로 Machine Learning 은 데이터를 정리하거나 나누거나 미래를 보이는 그 '함수' 혹은 '공식' 을 구하는 것 입니다. \n",
    "# 이번 경우는 상기 설명처럼, 데이터를 나누는 평면을 (공식을) 구하는 것이 목적입니다. \n",
    "\n",
    "# 아주 간단한 예로 SVM을 실습해 보겠습니다.\n",
    "\n",
    "# 수학 함수들을 사용하기 위헤 Numpy 모듈을 가져옵니다. \n",
    "import numpy as np\n",
    "# 해당 값들을 그래프로 표현하기 위헤 역시 Matplotlib을 가져옵니다. \n",
    "from matplotlib import pyplot as plt\n",
    "# 노트북에서 표현하기 위해 inline 합니다. \n",
    "%matplotlib inline\n",
    "\n",
    "#Step 1 - 먼저 데이터를 정의합니다. \n",
    "\n",
    "#Input data - Of the form [X value, Y value, Bias term]\n",
    "X = np.array([\n",
    "    [-2,4,-1],\n",
    "    [4,1,-1],\n",
    "    [1, 6, -1],\n",
    "    [2, 4, -1],\n",
    "    [6, 2, -1],\n",
    "])\n",
    "\n",
    "#Associated output labels - First 2 examples are labeled '-1' and last 3 are labeled '+1'\n",
    "y = np.array([-1,-1,1,1,1])\n",
    "\n",
    "# 위의 셈플 데이터를 그래프로 그려보겠습니다. \n",
    "# 모든 셈플 데이터에 대해, \n",
    "for d, sample in enumerate(X):\n",
    "    # 최초 2개의 negative samples 를 그리고, \n",
    "    if d < 2:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='_', linewidths=2)\n",
    "    # 나머지 positive samples 를 그립니다. \n",
    "    else:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='+', linewidths=2)\n",
    "\n",
    "# 적당히 두 갈래로 나눌 수 있도록 선을 예상하여 그러봅니다. \n",
    "plt.plot([-2,6],[6,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기계학습에서 목적 달성을 위해선 두 가지를 고려해야 합니다.**\n",
    "\n",
    " 1. 최소화할 것 (Loss Fuction)\n",
    " 2. 최적화할 것 (Objective Function) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 먼저 Loss Function 에는Hinge loss 함수를 사용하겠습니다. \n",
    "\n",
    "    1.  이 함수는 보통 위에서 이야기 한 분류모델의 Maximum-margin을 늘리기 위해 사용됩니다. \n",
    "    2.  https://github.com/llSourcell/Classifying_Data_Using_a_Support_Vector_Machine/blob/master/support_vector_machine_lesson.ipynb\n",
    "    3. 역시 상단 링크의 loss function 부분의 공식을 참고해 주세요. \n",
    "    4. 참고) +기호는 y*f(x)가 1이상인 경우 그냥 0으로 치겠다는 뜻 입니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화 할 공식 역시 상기 링크에 나와 있는데, **min(람다)(w)^2 + 시그마(1-y(x,w))** 로 표현됩니다. \n",
    "\n",
    "- 앞 부분 (min(람다)(w)^2) 을 regularizer (정규화공식) 라 칭하며, 뒷 부분은 loss 입니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "공식의 진행을 보시면 아시겠지만 이 경우에도 역시 Gradient Decent을 이용합니다, \n",
    "\n",
    "**1. 만약 셈플이 잘못 분류되었다면** => \n",
    "    - 앞 부분을 편미분 하고 뒷 부분 역시 편미분하여,\n",
    "    - 두 부분 모두의 기울기를 사용하여 w 즉 가중치를 업데이트합니다. \n",
    "    - 이 경우 공식은 w = w + n(yixi - 2(람다)w) 가 됩니다. 여기서 n 은 Learning Rate, 즉 학습 속도입니다. \n",
    "**2. 그러나 셈플이 잘 분류되었다면** =>\n",
    "    - 그냥 정규화공식의 기울기로 w 즉 가중치를 업데이트합니다. \n",
    "    - 이 경우 공식은 w = w + n(-2(람다)w) 가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "때문에 여기서 학습 속도란, 알고리즘이 경사 하강을 얼만큼씩 하는지이며 이 안엔 정규화공식이 포함되어 있습니다. \n",
    "\n",
    "**이 시점에서 앞 부분, 즉 정규화에 대한 명확한 이해가 중요합니다.** \n",
    "- 정규화공식은 Margin Maximization 과 Loss의 벨런스를 맞춥니다. \n",
    "- 마진 최대화가 목적인 것은 맞으나, \n",
    "    1. n 즉 학습속도가 너무 높은 경우, 알고리즘이 최적 포인트를 너무 빠르게 지나가버릴 수 있습니다. \n",
    "    2. n 즉 학습속도가 너무 느린 경우, 알고리즘이 평생 최적 포인트를 못 찾을 수 도 있습니다. \n",
    "    \n",
    "    \n",
    "- 또한 여기서 정규화공식은 트레이닝데이터에서의 오류와 테스트데이터사이에서의 오류의 벨런스도 맞추는데, \n",
    "    1. 너무 정규화한 경우 -> Overfit (오버피팅 되며 테스트데이터에서 높은 오차율을 내게 됩니다.)\n",
    "        - 쉽게 보자면 모든 트레이닝 데이터를 너무 완벽하게 학습해버려, 실제 모르는 데이터에서는 높은 오차율을 내게 됩니다. \n",
    "    2. 너무 정규화안한 경우 -> Underfit (너무 언더피팅되어 트레이닝데이터 자체에서 높은 오차율을 내게 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 이론을 기반으로 경사 하강법을 통해 두 가지 상태를 분류하는 하이퍼플레인을 구해보겠습니다.\n",
    "\n",
    "def svm_sgd_plot(X, Y):\n",
    "    # SVM의 가중치를 초기화합니다.\n",
    "    w = np.zeros(len(X[0]))\n",
    "    #The learning rate\n",
    "    # 학습속도\n",
    "    eta = 1\n",
    "    # 몇 번 반복할지를 정의합니다.\n",
    "    epochs = 100000\n",
    "    # 잘못 분류된 경우 이를 기록해, 시간이 지남에 따라 얼마나 오류가 줄어들고 있는지 확인하기 위한 리스트입니다.\n",
    "    errors = []\n",
    "\n",
    "    # 위의 이론을 기반으로 함수를 코딩합니다.\n",
    "    for epoch in range(1,epochs):\n",
    "        error = 0\n",
    "        for i, x in enumerate(X):\n",
    "            # 분류가 잘못되었을 경우의 가중치 업데이트\n",
    "            if (Y[i]*np.dot(X[i], w)) < 1:\n",
    "                w = w + eta * ( (X[i] * Y[i]) + (-2  *(1/epoch)* w) )\n",
    "                error = 1\n",
    "            # 분류가 잘 되었을 경우의 가중치 업데이트\n",
    "            else:\n",
    "                w = w + eta * (-2  *(1/epoch)* w)\n",
    "        errors.append(error)\n",
    "        \n",
    "\n",
    "    #잘못 분류된 횟수가 어떻게 줄어들고 있는지 확인하기 위해 그래프를 그려봅니다.\n",
    "    plt.plot(errors, '|')\n",
    "    plt.ylim(0.5,1.5)\n",
    "    plt.axes().set_yticklabels([])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Misclassified')\n",
    "    plt.show()\n",
    "    \n",
    "    return w\n",
    "\n",
    "# SVM알고리즘 자체는 수학만 확실하다면 고작 14줄의 코딩으로 완성됨을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apostsik/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/figure.py:98: MatplotlibDeprecationWarning: \n",
      "Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  \"Adding an axes using the same arguments as a previous axes \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = svm_sgd_plot(X,y)\n",
    "# 세대가 지남에 따라 점점 오류가 줄어들고 있음을 확인할 수 있습니다. \n",
    "# SVM이 점점 최적화된 하이퍼플래인을 찾아내고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.quiver.Quiver at 0x7f149b95af60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for d, sample in enumerate(X):\n",
    "    # - 셈플들을 그립니다.\n",
    "    if d < 2:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='_', linewidths=2)\n",
    "    # + 셈플들을 그립니다.\n",
    "    else:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='+', linewidths=2)\n",
    "\n",
    "# 테스트데이터를 추가합니다. \n",
    "plt.scatter(2,2, s=120, marker='_', linewidths=2, color='yellow')\n",
    "plt.scatter(4,3, s=120, marker='+', linewidths=2, color='blue')\n",
    "\n",
    "# svm_sgd() 가 계산해낸 하이퍼플래인을 그립니다. \n",
    "x2=[w[0],w[1],-w[1],w[0]]\n",
    "x3=[w[0],w[1],w[1],-w[0]]\n",
    "\n",
    "x2x3 =np.array([x2,x3])\n",
    "X,Y,U,V = zip(*x2x3)\n",
    "ax = plt.gca()\n",
    "ax.quiver(X,Y,U,V,scale=1, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Second Order Optimazation\n",
    "==\n",
    "- 영상) https://www.youtube.com/watch?v=UIFMLK2nj_w&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D&index=3\n",
    "- 상기 경사 하강법에서는 도함수 (First Order)만을 사용하였지만, 더 나아가 이계도함수 (Second Order)를 사용하여 최적화를 하는 방법을 알아보겠습니다. \n",
    "- 도함수는 어떠한 시점에서 해당 본 함수가 증가하고 있는지 혹은 감소하고 있는지를 나타내지만,\n",
    "- 이계도함수는 해당 시점에서 도함수가 증가하고 있는지 혹은 감소하고 있는지를 나타냅니다.  \n",
    "\n",
    "\n",
    "- 이계도함수는 표면의 곡률을 무시하지 않기 때문에, 경사 하강법보다 더 좋은 경우가 있을 수 있으나, 많은 계산량이 필요합니다.\n",
    "- 이번 예제는 실제로 안 해보셔도 괜찮습니다. (4.에서 바로 다시 다루기 때문.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 영상에서는 '뉴턴 방법'을 소개합니다. 뉴턴 방법은, 스칼라변수로 이루어져있고 미분 가능한 연속 함수를 푸는 여러가지 방법을 이야기합니다. \n",
    "\n",
    "1. Finding Roots (할선법)\n",
    "    - 이 방법은 결과적으로 x절편을 찾아갑니다.\n",
    "    - 영상에서 기울기를 찾고 해당 기울기가 갖는 x절편에서 다시 해당 위치의 기울기에서 x절편을 찾아가는 영상을 확인해보세요.\n",
    "2. Optimazation (이분법) \n",
    "    - 이 방법은 근이 반드시 존재하는 폐구간을 이분한 후, 이 중 근이 존재하는 하위 폐구간을 선택하는 것을 반복하여서 근을 찾는 알고리즘입니다.\n",
    "    - 역시 영상을 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n",
      "Tests passed.\n",
      "Tests passed.\n",
      "Root is at:  0\n",
      "f(x) at root is:  0\n",
      "Root is at:  0.6286669787778999\n",
      "f(x) at root is:  -1.8043344596208044e-12\n",
      "Root is at:  1\n",
      "f(x) at root is:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apostsik/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/scipy/optimize/zeros.py:169: RuntimeWarning: derivative was zero.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Finding Roots의 예제입니다.\n",
    "\"\"\"\n",
    "Newton's method\n",
    "Author: Daniel Homola\n",
    "Licence: BSD 3-clause\n",
    "\"\"\"\n",
    "\n",
    "from scipy.optimize import newton\n",
    "from sklearn.utils.testing import assert_almost_equal\n",
    "\n",
    "def f(x):\n",
    "    return 6*x**5-5*x**4-4*x**3+3*x**2\n",
    "def df(x):\n",
    "    return 30*x**4-20*x**3-12*x**2+6*x\n",
    "\n",
    "def dx(f, x):\n",
    "    return abs(0-f(x))\n",
    "    \n",
    "def newtons_method(f, df, x0, e, print_res=False):\n",
    "    delta = dx(f, x0)\n",
    "    while delta > e:\n",
    "        x0 = x0 - f(x0)/df(x0)\n",
    "        delta = dx(f, x0)\n",
    "    if print_res:\n",
    "        print ('Root is at: ', x0 )\n",
    "        print ('f(x) at root is: ', f(x0))\n",
    "    return x0\n",
    "\n",
    "def test_with_scipy(f, df, x0s, e):\n",
    "    for x0 in x0s:\n",
    "        my_newton = newtons_method(f, df, x0, e)\n",
    "        scipy_newton = newton(f, x0, df, tol=e)\n",
    "        assert_almost_equal(my_newton, scipy_newton, decimal=5)\n",
    "        print ('Tests passed.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # run test\n",
    "    x0s = [0, .5, 1]    \n",
    "    test_with_scipy(f, df, x0s, 1e-5)\n",
    "        \n",
    "    for x0 in x0s:\n",
    "        newtons_method(f, df, x0, 1e-10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimazation의 예제입니다. (하단 링크)\n",
    "\n",
    "# https://github.com/llSourcell/Second_Order_Optimization_Newtons_Method/blob/master/newtons_method_optimization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 \n",
    " - First Order Optimization 은 도함수를 이용하고 -> Jacaobian Matrix를 사용하며\n",
    " - Second Order Optimization 은 이계도함수를 이용하고 -> Hessian Matrix를 사용합니다. \n",
    "\n",
    "\n",
    "또한, 특정 상황에서는 이계도함수가 더 좋은 경우가 생길 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Logistic Regression (로지스틱 회귀)\n",
    "==\n",
    "영상) https://www.youtube.com/watch?v=D8alok2P468&index=4&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D\n",
    "\n",
    "- 로지스틱 회귀는 일반적으로 Classification에 이용됩니다. \n",
    "- 이번에는 특정한 데이터를 받아, 당뇨 환자를 구분하는 예제를 실습하여 볼 것인데, \n",
    "- 3.에서 나온 뉴턴 방법을 이용하여 로지스틱 선형회귀를 구현하여 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선형회귀는 연속적인 다음 값을 예측하지만, 로지스틱 회귀는 시그모이드함수를 이용하여 특정한 값에서 분류를 실시합니다. \n",
    "- 0 혹은 1 같은 확실한 분류를 위해 사용됩니다. (위의 예시처럼 당뇨가 있다 or 없다.)\n",
    "- 기본적인 공식은 1/(1+exp(-x))입니다. \n",
    "- exp(-x)을 사용하는 이유는 exp(-x)의 도함수는 exp(-x)라는 특성을 이용하기 위함입니다. \n",
    "\n",
    "\n",
    "- 결과적으로 Linear SVM과 로지스틱 회귀는 동일한 분류를 진행할 수 있습니다.\n",
    "- 또한 SVM과 로지스틱 회귀는 손실함수가 비슷하기 때문에 그 학습결과 또한 유사한 경향을 보이게 됩니다. \n",
    "- 다만, SVM은 Non-linear 한 분류도 진행할 수 있으며, 일반화에 유리하여 확장성에 유용합니다. \n",
    "- 그러나 로지스틱 회귀는 Bayesian 모델에서 사용될 수 있으며, 데이터의 갯수가 많다면 더 유리할 수 있습니다. \n",
    "- 이 부분은 Gaussian Kernel이 더해진 SVM을 학습할 시 더 자세하게 다루게 됩니다. \n",
    "\n",
    "\n",
    "이번 데이터는 몸무게, 키, 혈압을 기반으로 환자가 당뇨가 있을지 없을지에 대한 분류를 진행합니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MLE는 Maximum likelihood of estimation으로, 한 테스팅 데이터를 받았을 때 \n",
    "- NN등에서 만든 모델이 성공적으로 분류할 확률을 높히는 과정이라고 생각할 수 있습니다. \n",
    "- 이 경우 뉴턴방법을 사용하여, 로지스틱 회귀의 MLE를 진행하는 것 입니다. (Backpropagation과 같은 개념입니다.)\n",
    "\n",
    "\n",
    "- 뉴턴 방법은 경사 하강보다 더 빠르게 MLE를 이룰 수 있게 하여 줍니다.\n",
    "- 매 회차마다 경사 하강 더 효과적으로 진전하는 이유는, 이가 Hessian의 inverse를 계산하기 때문입니다. \n",
    "\n",
    "\n",
    "- 선택) 왜 퓨어한 확률이 아닌 log(odd)를 사용하는지 이해하고 넘어가면 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#matrix math\n",
    "import numpy as np\n",
    "#data manipulation\n",
    "import pandas as pd\n",
    "#matrix data structure\n",
    "from patsy import dmatrices\n",
    "#for error logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 함수를 정의합니다. \n",
    "def sigmoid(x):\n",
    "    '''Sigmoid function of x.'''\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 번호를 정하게 되면, 매번 같은 랜덤 숫자를 생성하게 됩니다. \n",
    "# 매번 같은 랜덤 숫자를 생성해야, 재생산과 디버깅에 효율적이기 때문에 시드 번호를 생성하는 것이 좋습니다.\n",
    "np.random.seed(0) # 시드 번호를 정합니다.\n",
    "\n",
    "##Step 1 - 모델의 parameters 를 정의합니다. (하이퍼 파라미터)\n",
    "\n",
    "## 알고리즘 세팅\n",
    "\n",
    "# 모델이 언제 학습을 멈출지를 정합니다. \n",
    "tol=1e-8 # convergence tolerance\n",
    "# 오버피팅을 막기 위해 l2-정규화를 진행합니다.\n",
    "\n",
    "# l1 정규화와 l2 정규화는 상당히 유명한 정규화 방법입니다. \n",
    "# l1 정규화는 단순히 가중치의 도합이며, (데이터셋이 적고 계산량이 많지 않으면 l1을)\n",
    "# l2 정규화는 가중치 제곱의 도합입니다. (데이터셋이 크고 계산량이 많으면 l2를 일반적으로 사용합니다.)\n",
    "lam = None\n",
    "# 최대 허용 반복횟수\n",
    "max_iter = 20 \n",
    "\n",
    "## 데이터 세팅\n",
    "# covariance 는 두 데이터가 어떻게 움직이는지에 대한 변수입니다.\n",
    "r = 0.95 # 양수이면 점점 가까워지며, 음수이면 점점 멀어집니다. 이 경우 점점 가까워져야 합니다. \n",
    "n = 1000 # 데이터셋의 사이즈입니다.  \n",
    "sigma = 1 # 데이터가 얼마나 넓게 퍼진지에 대한 변수입니다. \n",
    "\n",
    "## 모델 세팅\n",
    "beta_x, beta_z, beta_v = -4, .9, 1 # true beta coefficients\n",
    "var_x, var_z, var_v = 1, 1, 4 # variances of inputs\n",
    "\n",
    "## 모델의 모양을 정의합니다. \n",
    "formula = 'y ~ x + z + v + np.exp(x) + I(v**2 + z)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/llSourcell/logistic_regression_newtons_method/blob/master/NewtonCode.ipynb\n",
    "\n",
    "## 이후는 해당 노트북을 끝까지 참고합니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
